{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30c81f15",
      "metadata": {
        "id": "30c81f15"
      },
      "source": [
        "### OpenCV : Core Tools for VisionExtract"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf7405d",
      "metadata": {
        "id": "6bf7405d"
      },
      "source": [
        "- What is OpenCV?\n",
        "  OpenCV (Open Source Computer Vision Library) is a powerful, open-source library for real-time image and video processing. In VisionExtract, it's essential for loading, preprocessing, and manipulating images before they reach the deep learning model.\n",
        "\n",
        "- Key Capabilities:\n",
        "\n",
        "  - Image I/O (read/write various formats)\n",
        "\n",
        "  - Image transformations (resize, crop, rotate)\n",
        "\n",
        "  - Color space conversions (RGB ↔ grayscale ↔ HSV)\n",
        "\n",
        "  - Filtering and edge detection\n",
        "\n",
        "  - Morphological operations (erosion, dilation)\n",
        "\n",
        "  - Fast C++ backend with Python bindings\n",
        "\n",
        "- Installation:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "_QsF2QCAdQz8"
      },
      "id": "_QsF2QCAdQz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12071321",
      "metadata": {
        "id": "12071321"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6038f2ee",
      "metadata": {
        "id": "6038f2ee"
      },
      "source": [
        "#### 1. Loading and Displaying Images\n",
        "- `OpenCV` loads images as NumPy arrays where pixel values range from 0-255. It uses BGR format (not RGB) by default."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load image in BGR format (default)\n",
        "image_bgr = cv2.imread('/content/drive/MyDrive/AI_Vision_Extract_Nov25/notebooks/image.jpg')\n",
        "\n",
        "# Important: Convert BGR to RGB for proper visualization\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display image\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(image_rgb)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Check image shape and data type\n",
        "print(f\"Image shape: {image_rgb.shape}\")  # (height, width, channels)\n",
        "print(f\"Data type: {image_rgb.dtype}\")    # uint8\n",
        "print(f\"Value range: {image_rgb.min()}-{image_rgb.max()}\")  # 0-255"
      ],
      "metadata": {
        "id": "qcRTgbdqdIdp"
      },
      "id": "qcRTgbdqdIdp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9df42934",
      "metadata": {
        "id": "9df42934"
      },
      "source": [
        "- Key Points:\n",
        "\n",
        "    - Always convert BGR → RGB when working with matplotlib or sending to models\n",
        "\n",
        "    - Image shape is (height, width, channels), not (width, height, channels)\n",
        "\n",
        "    - Pixel values are typically uint8 (0-255), but models expect normalized float32 (0-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5dda6b",
      "metadata": {
        "id": "4e5dda6b"
      },
      "source": [
        "#### 2. Image Resizing - Critical for Model Input\n",
        "- Deep learning models require fixed input sizes. COCO-2017 images have varying resolutions, so resizing standardizes them to 512×512."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Method-1: Simple Resize\n",
        "image_resized = cv2.resize(image_rgb, (512, 512))\n",
        "print(f\"Resized shape: {image_resized.shape}\")\n",
        "\n",
        "# Method 2: Resize with aspect ratio preservation (padding)\n",
        "height, width = image_rgb.shape[:2]\n",
        "scale = 512 / max(height, width)\n",
        "new_width = int(width * scale)\n",
        "new_height = int(height * scale)\n",
        "\n",
        "# Resize\n",
        "image_resized = cv2.resize(image_rgb, (new_width, new_height))\n",
        "\n",
        "# Create canvas and pad with black\n",
        "canvas = np.zeros((512, 512, 3), dtype=np.uint8)\n",
        "y_offset = (512 - new_height) // 2\n",
        "x_offset = (512 - new_width) // 2\n",
        "canvas[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = image_resized\n",
        "\n",
        "print(f\"Padded shape: {canvas.shape}\")  # (512, 512, 3)"
      ],
      "metadata": {
        "id": "yZTCXo5udJox"
      },
      "id": "yZTCXo5udJox",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fcac8375",
      "metadata": {
        "id": "fcac8375"
      },
      "source": [
        "- Why This Matters:\n",
        "\n",
        "    - Simple resize: Fast but distorts aspect ratio (avoid if possible)\n",
        "\n",
        "    - Padding: Preserves aspect ratio but adds black borders (better for object detection)\n",
        "\n",
        "    - For VisionExtract, simple 512×512 resize works fine since we have masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f08c2b",
      "metadata": {
        "id": "a1f08c2b"
      },
      "source": [
        "#### 3. Color Space Conversion\n",
        "- Some models work better with grayscale, HSV helps with lighting invariance, etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB to Grayscale\n",
        "image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
        "print(f\"Grayscale shape: {image_gray.shape}\")  # (height, width) - NO channel dimension\n",
        "\n",
        "# RGB to HSV (useful for color-based segmentation)\n",
        "image_hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "# Visualize all three\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].imshow(image_rgb)\n",
        "axes[0].set_title(\"RGB\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(image_gray, cmap='gray')\n",
        "axes[1].set_title(\"Grayscale\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(image_hsv)\n",
        "axes[2].set_title(\"HSV\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f2JrN0ltdKrV"
      },
      "id": "f2JrN0ltdKrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7279a01",
      "metadata": {
        "id": "b7279a01"
      },
      "source": [
        "#### 4. Image Normalization - Prepare for Models\n",
        "- Critical Step: Models expect pixel values in range [-1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Simple normalization (0-1)\n",
        "image_normalized = image_rgb.astype(np.float32) / 255.0\n",
        "print(f\"Value range: {image_normalized.min()}-{image_normalized.max()}\")  # 0.0-1.0\n",
        "\n",
        "# Method 2: ImageNet normalization (standard for pre-trained models)\n",
        "# Pre-trained models (ResNet, VGG, etc.) expect this normalization\n",
        "mean = np.array([0.485, 0.456, 0.406])  # ImageNet mean (RGB)\n",
        "std = np.array([0.229, 0.224, 0.225])   # ImageNet std (RGB)\n",
        "\n",
        "image_normalized = (image_normalized - mean) / std\n",
        "print(f\"Value range: {image_normalized.min():.3f}-{image_normalized.max():.3f}\")"
      ],
      "metadata": {
        "id": "6jGfuH_DdLqg"
      },
      "id": "6jGfuH_DdLqg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0ec3f917",
      "metadata": {
        "id": "0ec3f917"
      },
      "source": [
        "- Why ImageNet normalization?\n",
        "    - Pre-trained encoders (ResNet50, etc.) were trained on ImageNet with this normalization\n",
        "    - Using it ensures the model receives inputs in the same scale as training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66b0c1f3",
      "metadata": {
        "id": "66b0c1f3"
      },
      "source": [
        "- Passing non-normalized vs normalized image to model\n",
        "    - ❌ Without normalization: Model sees pixel values 0-255 (extreme range)\n",
        "    - ✓ With normalization: Model sees values -2.0 to 2.0 (expected range)\n",
        "    - This improves gradient flow and training stability!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee07950",
      "metadata": {
        "id": "fee07950"
      },
      "source": [
        "#### 5. Reading and Processing Masks\n",
        "- For VisionExtract: Masks are grayscale images where pixel values represent class labels."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load mask in grayscale (single channel)\n",
        "mask = cv2.imread('/content/drive/MyDrive/AI_Vision_Extract_Nov25/notebooks/image.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "if mask is None:\n",
        "    raise FileNotFoundError(\"mask.jpg not found or cannot be loaded.\")\n",
        "print(f\"Mask shape: {mask.shape}\")  # (height, width)\n",
        "print(f\"Unique values in mask: {np.unique(mask)}\")  # Check all pixel values\n",
        "\n",
        "# Convert multi-class mask to binary (subject=1, background=0)\n",
        "binary_mask = (mask > 0).astype(np.uint8)  # 0 and 1\n",
        "print(f\"Binary mask unique values: {np.unique(binary_mask)}\")  # [0, 1]\n",
        "\n",
        "# Visualize the images and masks\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].imshow(image_rgb)\n",
        "axes[0].set_title(\"Image\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(mask, cmap='gray')\n",
        "axes[1].set_title(\"Multi-class Mask\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(binary_mask, cmap='gray')\n",
        "axes[2].set_title(\"Binary Mask\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QImMKWdGdMpy"
      },
      "id": "QImMKWdGdMpy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1f2a5ff7",
      "metadata": {
        "id": "1f2a5ff7"
      },
      "source": [
        "#### 6. Morphological Operations - Post-processing\n",
        "- After model prediction, morphological operations clean up noisy masks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Erosion: Removes small white noise\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "mask_eroded = cv2.erode(binary_mask, kernel, iterations=1)\n",
        "\n",
        "# Dilation: Fills small holes\n",
        "mask_dilated = cv2.dilate(binary_mask, kernel, iterations=1)\n",
        "\n",
        "# Closing: Dilation followed by erosion (fills holes)\n",
        "mask_closed = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# Opening: Erosion followed by dilation (removes noise)\n",
        "mask_opened = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "# Visualize effects\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "axes[0, 0].imshow(binary_mask, cmap='gray')\n",
        "axes[0, 0].set_title(\"Original\")\n",
        "\n",
        "axes[0, 1].imshow(mask_eroded, cmap='gray')\n",
        "axes[0, 1].set_title(\"Eroded\")\n",
        "\n",
        "axes[0, 2].imshow(mask_dilated, cmap='gray')\n",
        "axes[0, 2].set_title(\"Dilated\")\n",
        "\n",
        "axes[1, 0].imshow(mask_opened, cmap='gray')\n",
        "axes[1, 0].set_title(\"Opened\")\n",
        "\n",
        "axes[1, 1].imshow(mask_closed, cmap='gray')\n",
        "axes[1, 1].set_title(\"Closed\")\n",
        "\n",
        "# Apply mask to image (isolate subject)\n",
        "isolated = np.zeros_like(image_rgb)\n",
        "isolated[mask_closed > 0] = image_rgb[mask_closed > 0]\n",
        "axes[1, 2].imshow(isolated)\n",
        "axes[1, 2].set_title(\"Isolated Subject\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f8Yp2gOadN2a"
      },
      "id": "f8Yp2gOadN2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "960cb1ac",
      "metadata": {
        "id": "960cb1ac"
      },
      "source": [
        "- Key Teaching Points:\n",
        "\n",
        "    - Erosion: Shrinks white regions, removes thin features\n",
        "\n",
        "    - Dilation: Expands white regions, fills holes\n",
        "\n",
        "    - Closing: Good for removing small noise\n",
        "\n",
        "    - Opening: Good for removing thin artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c435e86",
      "metadata": {
        "id": "1c435e86"
      },
      "source": [
        "#### 7. Image Filtering - Blur and Edge Detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Blur: Smooth image (reduce noise)\n",
        "blurred = cv2.GaussianBlur(image_rgb, (5, 5), 1.0)\n",
        "\n",
        "# Canny Edge Detection\n",
        "edges = cv2.Canny(image_gray, 100, 200)\n",
        "\n",
        "# Sobel operators (gradient-based edge detection)\n",
        "sobel_x = cv2.Sobel(image_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "sobel_y = cv2.Sobel(image_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "axes[0, 0].imshow(image_rgb)\n",
        "axes[0, 0].set_title(\"Original\")\n",
        "\n",
        "axes[0, 1].imshow(blurred)\n",
        "axes[0, 1].set_title(\"Gaussian Blur\")\n",
        "\n",
        "axes[1, 0].imshow(edges, cmap='gray')\n",
        "axes[1, 0].set_title(\"Canny Edges\")\n",
        "\n",
        "axes[1, 1].imshow(sobel_magnitude, cmap='gray')\n",
        "axes[1, 1].set_title(\"Sobel Magnitude\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qYzy4QIUdPF-"
      },
      "id": "qYzy4QIUdPF-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}